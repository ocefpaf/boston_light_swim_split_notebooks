{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='float: left' width=\"150px\" src=\"http://bostonlightswim.org/wp/wp-content/uploads/2011/08/BLS-front_4-color.jpg\">\n",
    "<br><br>\n",
    "\n",
    "## [The Boston Light Swim](http://bostonlightswim.org/)\n",
    "\n",
    "### Fetch Sea Surface Temperature time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "import iris\n",
    "import cf_units\n",
    "\n",
    "from owslib import fes\n",
    "from datetime import datetime, timedelta\n",
    "from utilities import CF_names, start_log\n",
    "\n",
    "# Today +- 4 days\n",
    "today = datetime.utcnow()\n",
    "today = today.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "start = today - timedelta(days=4)\n",
    "stop = today + timedelta(days=4)\n",
    "\n",
    "# Boston harbor.\n",
    "spacing = 0.25\n",
    "# [lon_min, lat_min, lon_max, lat_max]\n",
    "bbox = [-71.05-spacing, 42.28-spacing,\n",
    "        -70.82+spacing, 42.38+spacing]\n",
    "bbox_crs = fes.BBox(bbox, crs='urn:ogc:def:crs:OGC:1.3:CRS84')\n",
    "\n",
    "# CF-names.\n",
    "sos_name = 'sea_water_temperature'\n",
    "name_list = CF_names[sos_name]\n",
    "\n",
    "# Units.\n",
    "units = cf_units.Unit('celsius')\n",
    "\n",
    "# Logging.\n",
    "run_name = 'latest'  # '{:%Y-%m-%d}'.format(stop) to use the date.\n",
    "log = start_log(start, stop, bbox, run_name)\n",
    "\n",
    "# Config.\n",
    "fname = os.path.join(run_name, 'config.pkl')\n",
    "config = dict(start=start,\n",
    "              stop=stop,\n",
    "              bbox=bbox,\n",
    "              name_list=name_list,\n",
    "              units=units,\n",
    "              run_name=run_name)\n",
    "\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utilities import fes_date_filter\n",
    "\n",
    "kw = dict(wildCard='*',\n",
    "          escapeChar='\\\\',\n",
    "          singleChar='?',\n",
    "          propertyname='apiso:AnyText')\n",
    "\n",
    "or_filt = fes.Or([fes.PropertyIsLike(literal=('*%s*' % val), **kw)\n",
    "                  for val in name_list])\n",
    "\n",
    "# Exclude ROMS Averages and History files.\n",
    "not_filt = fes.Not([fes.PropertyIsLike(literal='*Averages*', **kw)])\n",
    "\n",
    "begin, end = fes_date_filter(start, stop)\n",
    "filter_list = [fes.And([bbox_crs, begin, end, or_filt, not_filt])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def service_urls(records, services=['urn:x-esri:specification:ServiceType:odp:url']):\n",
    "    \"\"\"\n",
    "    Extract service_urls of a specific type (DAP, SOS) from csw records.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for service in services:\n",
    "        for key, rec in records.items():\n",
    "            url = next((d['url'] for d in rec.references if\n",
    "                        d['scheme'] == service), None)\n",
    "            if url is not None:\n",
    "                urls.append(url)\n",
    "        urls = sorted(set(urls))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from owslib.csw import CatalogueServiceWeb\n",
    "\n",
    "# Logging info.\n",
    "fmt = '{:*^64}'.format\n",
    "log.info(fmt(' Catalog information '))\n",
    "log.info(fmt(' CSW '))\n",
    "\n",
    "# http://data.ioos.us/csw is too old and does not support CRS.\n",
    "endpoints = ['http://www.ngdc.noaa.gov/geoportal/csw',\n",
    "             'http://geoport.whoi.edu/csw']\n",
    "\n",
    "services = ['OPeNDAP:OPeNDAP',\n",
    "           'urn:x-esri:specification:ServiceType:odp:url']\n",
    "\n",
    "dap_urls = []\n",
    "sos_urls = []\n",
    "for endpoint in endpoints:\n",
    "    log.info(\"URL: {}\".format(endpoint))\n",
    "    \n",
    "    csw = CatalogueServiceWeb(endpoint, timeout=60)\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=1000, esn='full')\n",
    "    # Check for the strings in: https://raw.githubusercontent.com/OSGeo/Cat-Interop/master/LinkPropertyLookupTable.csv\n",
    "    dap_urls.extend(service_urls(csw.records, services=services))\n",
    "    sos_urls.extend(service_urls(csw.records, services=services))\n",
    "\n",
    "    log.info(\"CSW version: {}\".format(csw.version))\n",
    "    log.info(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    \n",
    "    for rec, item in csw.records.items():\n",
    "        log.info('{}'.format(item.title))\n",
    "    log.info(fmt(' SOS '))\n",
    "    for url in sos_urls:\n",
    "        log.info('{}'.format(url))\n",
    "    log.info(fmt(' DAP '))\n",
    "    for url in dap_urls:\n",
    "        log.info('{}.html'.format(url))\n",
    "\n",
    "# Get only unique endpoints.\n",
    "dap_urls = list(set(dap_urls))\n",
    "sos_urls = list(set(sos_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utilities import is_station\n",
    "\n",
    "# Filter out some station endpoints.\n",
    "non_stations = []\n",
    "for url in dap_urls:\n",
    "    try:\n",
    "        if not is_station(url):\n",
    "            non_stations.append(url)\n",
    "    except RuntimeError as e:\n",
    "        log.warn(\"Could not access URL {}. {!r}\".format(url, e))\n",
    "\n",
    "dap_urls = non_stations\n",
    "\n",
    "log.info(fmt(' Filtered DAP '))\n",
    "for url in dap_urls:\n",
    "    log.info('{}.html'.format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NdbcSos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyoos.collectors.ndbc.ndbc_sos import NdbcSos\n",
    "\n",
    "collector_ndbc = NdbcSos()\n",
    "\n",
    "collector_ndbc.set_bbox(bbox)\n",
    "collector_ndbc.end_time = stop\n",
    "collector_ndbc.start_time = start\n",
    "collector_ndbc.variables = [sos_name]\n",
    "\n",
    "ofrs = collector_ndbc.server.offerings\n",
    "title = collector_ndbc.server.identification.title\n",
    "log.info(fmt(' NDBC Collector offerings '))\n",
    "log.info('{}: {} offerings'.format(title, len(ofrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utilities import collector2table, to_html, get_ndbc_longname\n",
    "\n",
    "ndbc = collector2table(collector=collector_ndbc)\n",
    "\n",
    "names = []\n",
    "for s in ndbc['station']:\n",
    "    try:\n",
    "        name = get_ndbc_longname(s)\n",
    "    except ValueError:\n",
    "        name = s\n",
    "    names.append(name)\n",
    "\n",
    "ndbc['name'] = names\n",
    "\n",
    "ndbc.set_index('name', inplace=True)\n",
    "to_html(ndbc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoopsSoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyoos.collectors.coops.coops_sos import CoopsSos\n",
    "\n",
    "collector_coops = CoopsSos()\n",
    "\n",
    "collector_coops.set_bbox(bbox)\n",
    "collector_coops.end_time = stop\n",
    "collector_coops.start_time = start\n",
    "collector_coops.variables = [sos_name]\n",
    "\n",
    "ofrs = collector_coops.server.offerings\n",
    "title = collector_coops.server.identification.title\n",
    "log.info(fmt(' Collector offerings '))\n",
    "log.info('{}: {} offerings'.format(title, len(ofrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utilities import get_coops_metadata\n",
    "\n",
    "coops = collector2table(collector=collector_coops)\n",
    "\n",
    "names = []\n",
    "for s in coops['station']:\n",
    "    try:\n",
    "        name = get_coops_metadata(s)[0]\n",
    "    except ValueError:\n",
    "        name = s\n",
    "    names.append(name)\n",
    "\n",
    "coops['name'] = names\n",
    "\n",
    "coops.set_index('name', inplace=True)\n",
    "to_html(coops.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join CoopsSoS and NdbcSos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import concat\n",
    "\n",
    "all_obs = concat([coops, ndbc])\n",
    "\n",
    "to_html(all_obs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = '{}-all_obs.csv'.format(run_name)\n",
    "fname = os.path.join(run_name, fname)\n",
    "all_obs.to_csv(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the observed data series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from owslib.ows import ExceptionReport\n",
    "from utilities import pyoos2df, save_timeseries\n",
    "\n",
    "iris.FUTURE.netcdf_promote = True\n",
    "\n",
    "log.info(fmt(' Observations '))\n",
    "outfile = '{}-OBS_DATA.nc'.format(run_name)\n",
    "outfile = os.path.join(run_name, outfile)\n",
    "\n",
    "\n",
    "log.info(fmt(' Downloading to file {} '.format(outfile)))\n",
    "data = dict()\n",
    "col = 'sea_water_temperature (C)'\n",
    "for station in all_obs.index:\n",
    "    try:\n",
    "        idx = all_obs['station'][station]\n",
    "        df = pyoos2df(collector_ndbc, idx, df_name=station)\n",
    "        if df.empty:\n",
    "            df = pyoos2df(collector_coops, idx, df_name=station)\n",
    "        data.update({idx: df[col]})\n",
    "    except ExceptionReport as e:\n",
    "        log.warning(\"[{}] {}:\\n{}\".format(idx, station, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform 1-hour time base for model/data comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import date_range\n",
    "\n",
    "index = date_range(start=start, end=stop, freq='1H')\n",
    "for k, v in data.iteritems():\n",
    "    data[k] = v.reindex(index=index, limit=1, method='nearest')\n",
    "\n",
    "obs_data = DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comment = \"Several stations from http://opendap.co-ops.nos.noaa.gov\"\n",
    "kw = dict(longitude=all_obs.lon,\n",
    "          latitude=all_obs.lat,\n",
    "          station_attr=dict(cf_role=\"timeseries_id\"),\n",
    "          cube_attr=dict(featureType='timeSeries',\n",
    "                         Conventions='CF-1.6',\n",
    "                         standard_name_vocabulary='CF-1.6',\n",
    "                         cdm_data_type=\"Station\",\n",
    "                         comment=comment,\n",
    "                         url=url))\n",
    "\n",
    "save_timeseries(obs_data, outfile=outfile,\n",
    "                standard_name=sos_name, **kw)\n",
    "\n",
    "to_html(obs_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop discovered models and save the nearest time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utilities.tardis import _get_surface_idx\n",
    "\n",
    "def get_surface(cube):\n",
    "    conventions = cube.attributes.get('Conventions', 'None')\n",
    "\n",
    "    idx = _get_surface_idx(cube)\n",
    "    if cube.ndim == 4 or 'UGRID' in conventions.upper():\n",
    "        return cube[:, int(idx), ...]\n",
    "    elif cube.ndim == 3 and 'UGRID' not in conventions.upper():\n",
    "        return cube[int(idx), ...]\n",
    "    else:\n",
    "        msg = \"Cannot find the surface for cube {!r}\".format\n",
    "        raise ValueError(msg(cube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from iris.exceptions import (CoordinateNotFoundError, ConstraintMismatchError,\n",
    "                             MergeError)\n",
    "from utilities import (quick_load_cubes, proc_cube, is_model, get_model_name)\n",
    "\n",
    "log.info(fmt(' Models '))\n",
    "cubes = dict()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")  # Suppress iris warnings.\n",
    "    for k, url in enumerate(dap_urls):\n",
    "        log.info('\\n[Reading url {}/{}]: {}'.format(k+1, len(dap_urls), url))\n",
    "        try:\n",
    "            cube = quick_load_cubes(url, name_list,\n",
    "                                    callback=None, strict=True)\n",
    "            if is_model(cube):\n",
    "                cube = proc_cube(cube, bbox=bbox,\n",
    "                                 time=(start, stop), units=units)\n",
    "            else:\n",
    "                log.warning(\"[Not model data]: {}\".format(url))\n",
    "                continue\n",
    "            cube = get_surface(cube)\n",
    "            mod_name, model_full_name = get_model_name(cube, url)\n",
    "            cubes.update({mod_name: cube})\n",
    "        except (RuntimeError, ValueError,\n",
    "                ConstraintMismatchError, CoordinateNotFoundError,\n",
    "                IndexError) as e:\n",
    "            log.warning('Cannot get cube for: {}\\n{}'.format(url, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from iris.pandas import as_series\n",
    "from utilities import (make_tree, get_nearest_water,\n",
    "                       add_station, ensure_timeseries, remove_ssh)\n",
    "\n",
    "for mod_name, cube in cubes.items():\n",
    "    fname = '{}-{}.nc'.format(run_name, mod_name)\n",
    "    fname = os.path.join(run_name, fname)\n",
    "    log.info(fmt(' Downloading to file {} '.format(fname)))\n",
    "    try:\n",
    "        tree, lon, lat = make_tree(cube)\n",
    "    except CoordinateNotFoundError as e:\n",
    "        log.warning('Cannot make KDTree for: {}'.format(mod_name))\n",
    "        continue\n",
    "    # Get model series at observed locations.\n",
    "    raw_series = dict()\n",
    "    for station, obs in all_obs.iterrows():\n",
    "        try:\n",
    "            kw = dict(k=10, max_dist=0.08, min_var=0.01)\n",
    "            args = cube, tree, obs.lon, obs.lat\n",
    "            try:\n",
    "                series, dist, idx = get_nearest_water(*args, **kw)\n",
    "            except RuntimeError as e:\n",
    "                log.info('Cannot download {!r}.\\n{}'.format(cube, e))\n",
    "                series = None\n",
    "        except ValueError as e:\n",
    "            status = \"No Data\"\n",
    "            log.info('[{}] {}'.format(status, obs.name))\n",
    "            continue\n",
    "        if not series:\n",
    "            status = \"Land   \"\n",
    "        else:\n",
    "            raw_series.update({obs['station']: series})\n",
    "            series = as_series(series)\n",
    "            status = \"Water  \"\n",
    "        log.info('[{}] {}'.format(status, obs.name))\n",
    "    if raw_series:  # Save cube.\n",
    "        for station, cube in raw_series.items():\n",
    "            cube = add_station(cube, station)\n",
    "            cube = remove_ssh(cube)\n",
    "        try:\n",
    "            cube = iris.cube.CubeList(raw_series.values()).merge_cube()\n",
    "        except MergeError as e:\n",
    "            log.warning(e)\n",
    "        ensure_timeseries(cube)\n",
    "        iris.save(cube, fname)\n",
    "        del cube\n",
    "    log.info('Finished processing [{}]'.format(mod_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elapsed = time.time() - start_time\n",
    "log.info('{:.2f} minutes'.format(elapsed/60.))\n",
    "log.info('EOF')\n",
    "\n",
    "with open('{}/log.txt'.format(run_name)) as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
