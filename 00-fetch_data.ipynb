{
 "metadata": {
  "kernelspec": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "display_name": "IOOS (Python 2)",
   "language": "python",
   "name": "ioos_python2"
  },
  "name": "",
  "signature": "sha256:997f3e123c03fb98ccf5142e88619a52d08c60f1f8a24f46f515609fce972a73"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img style='float: left' width=\"150px\" src=\"http://bostonlightswim.org/wp/wp-content/uploads/2011/08/BLS-front_4-color.jpg\">\n",
      "<br><br>\n",
      "\n",
      "## [The Boston Light Swim](http://bostonlightswim.org/)\n",
      "\n",
      "### Fetch Sea Surface Temperature time-series data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "\n",
      "start_time = time.time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext watermark\n",
      "%watermark --githash --machine --python --packages iris,pyoos,owslib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Save configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "try:\n",
      "    import cPickle as pickle\n",
      "except ImportError:\n",
      "    import pickle\n",
      "\n",
      "import iris\n",
      "from datetime import datetime\n",
      "from utilities import CF_names, fetch_range, start_log\n",
      "\n",
      "# 2-weeks of data.\n",
      "kw = dict(start=datetime(2015, 8, 8), days=14)\n",
      "start, stop = fetch_range(**kw)\n",
      "\n",
      "# Boston harbor.\n",
      "spacing = 0.5\n",
      "[-71.55, -70.32, 41.78, 42.88]\n",
      "bbox = [-71.05-spacing, 42.28-spacing,\n",
      "        -70.82+spacing, 42.38+spacing]\n",
      "\n",
      "# CF-names.\n",
      "sos_name = 'sea_water_temperature'\n",
      "name_list = CF_names[sos_name]\n",
      "\n",
      "# Units.\n",
      "units = iris.unit.Unit('celsius')\n",
      "\n",
      "# Logging.\n",
      "run_name = '{:%Y-%m-%d}'.format(stop)\n",
      "log = start_log(start, stop, bbox)\n",
      "\n",
      "# Config.\n",
      "fname = os.path.join(run_name, 'config.pkl')\n",
      "config = dict(start=start,\n",
      "              stop=stop,\n",
      "              bbox=bbox,\n",
      "              name_list=name_list,\n",
      "              units=units,\n",
      "              run_name=run_name)\n",
      "\n",
      "with open(fname, 'wb') as f:\n",
      "    pickle.dump(config, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib import fes\n",
      "from utilities import fes_date_filter\n",
      "\n",
      "kw = dict(wildCard='*',\n",
      "          escapeChar='\\\\',\n",
      "          singleChar='?',\n",
      "          propertyname='apiso:AnyText')\n",
      "\n",
      "or_filt = fes.Or([fes.PropertyIsLike(literal=('*%s*' % val), **kw)\n",
      "                  for val in name_list])\n",
      "\n",
      "# Exclude ROMS Averages and History files.\n",
      "not_filt = fes.Not([fes.PropertyIsLike(literal='*Averages*', **kw)])\n",
      "\n",
      "begin, end = fes_date_filter(start, stop)\n",
      "filter_list = [fes.And([fes.BBox(bbox), begin, end, or_filt, not_filt])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib.csw import CatalogueServiceWeb\n",
      "\n",
      "endpoint = 'http://www.ngdc.noaa.gov/geoportal/csw'\n",
      "csw = CatalogueServiceWeb(endpoint, timeout=60)\n",
      "csw.getrecords2(constraints=filter_list, maxrecords=1000, esn='full')\n",
      "\n",
      "fmt = '{:*^64}'.format\n",
      "log.info(fmt(' Catalog information '))\n",
      "log.info(\"URL: {}\".format(endpoint))\n",
      "log.info(\"CSW version: {}\".format(csw.version))\n",
      "log.info(\"Number of datasets available: {}\".format(len(csw.records.keys())))"
     ],
     "language": "python",
     "metadata": {
      "code_folding": []
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import service_urls\n",
      "\n",
      "dap_urls = service_urls(csw.records, service='odp:url')\n",
      "sos_urls = service_urls(csw.records, service='sos:url')\n",
      "\n",
      "# Work around https://github.com/ioos/secoora/issues/184:\n",
      "dap_urls = [url for url in dap_urls if 'G1_SST_GLOBAL' not in url]\n",
      "\n",
      "log.info(fmt(' CSW '))\n",
      "for rec, item in csw.records.items():\n",
      "    log.info('{}'.format(item.title))\n",
      "\n",
      "log.info(fmt(' SOS '))\n",
      "for url in sos_urls:\n",
      "    log.info('{}'.format(url))\n",
      "\n",
      "log.info(fmt(' DAP '))\n",
      "for url in dap_urls:\n",
      "    log.info('{}.html'.format(url))"
     ],
     "language": "python",
     "metadata": {
      "code_folding": []
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "original = dap_urls[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import is_station\n",
      "\n",
      "# Filter out some station endpoints.\n",
      "non_stations = []\n",
      "for url in dap_urls:\n",
      "    try:\n",
      "        if not is_station(url):\n",
      "            non_stations.append(url)\n",
      "    except RuntimeError as e:\n",
      "        log.warn(\"Could not access URL {}. {!r}\".format(url, e))\n",
      "\n",
      "dap_urls = non_stations\n",
      "\n",
      "log.info(fmt(' Filtered DAP '))\n",
      "for url in dap_urls:\n",
      "    log.info('{}.html'.format(url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyoos.collectors.coops.coops_sos import CoopsSos\n",
      "\n",
      "collector = CoopsSos()\n",
      "\n",
      "collector.end_time = stop\n",
      "collector.start_time = start\n",
      "collector.variables = [sos_name]\n",
      "\n",
      "ofrs = collector.server.offerings\n",
      "title = collector.server.identification.title\n",
      "log.info(fmt(' Collector offerings '))\n",
      "log.info('{}: {} offerings'.format(title, len(ofrs)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import read_csv\n",
      "from utilities import sos_request\n",
      "\n",
      "params = dict(observedProperty=sos_name,\n",
      "              eventTime=start.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
      "              featureOfInterest='BBOX:{0},{1},{2},{3}'.format(*bbox),\n",
      "              offering='urn:ioos:network:NOAA.NOS.CO-OPS:MetActive')\n",
      "\n",
      "uri = 'http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS'\n",
      "url = sos_request(uri, **params)\n",
      "all_obs = read_csv(url)\n",
      "\n",
      "log.info('SOS URL request: {}'.format(url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean the DataFrame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import get_coops_metadata, to_html\n",
      "\n",
      "columns = {'sensor_id': 'sensor',\n",
      "           'station_id': 'station',\n",
      "           'latitude (degree)': 'lat',\n",
      "           'longitude (degree)': 'lon',\n",
      "           'sea_water_temperature (C)': sos_name}\n",
      "\n",
      "all_obs.rename(columns=columns, inplace=True)\n",
      "\n",
      "all_obs['sensor'] = [s.split(':')[-1] for s in all_obs['sensor']]\n",
      "all_obs['station'] = [s.split(':')[-1] for s in all_obs['station']]\n",
      "\n",
      "names = []\n",
      "for s in all_obs['station']:\n",
      "    try:\n",
      "        name = get_coops_metadata(s)[0]\n",
      "    except ValueError:\n",
      "        name = s\n",
      "    names.append(name)\n",
      "all_obs['name'] = names\n",
      "\n",
      "all_obs.set_index('name', inplace=True)\n",
      "to_html(all_obs.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Uniform 6-min time base for model/data comparison"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import DataFrame\n",
      "from owslib.ows import ExceptionReport\n",
      "from utilities import coops2df, save_timeseries\n",
      "\n",
      "iris.FUTURE.netcdf_promote = True\n",
      "\n",
      "log.info(fmt(' Observations '))\n",
      "outfile = '{:%Y-%m-%d}-OBS_DATA.nc'.format(stop)\n",
      "outfile = os.path.join(run_name, outfile)\n",
      "\n",
      "log.info(fmt(' Downloading to file {} '.format(outfile)))\n",
      "data, bad_station = dict(), []\n",
      "for station in all_obs.index:\n",
      "    try:\n",
      "        idx = all_obs['station'][station]\n",
      "        df = coops2df(collector, idx, df_name=station)\n",
      "        col = 'sea_water_temperature (C)'\n",
      "        data.update({idx: df[col]})\n",
      "    except ExceptionReport as e:\n",
      "        bad_station.append(idx)\n",
      "        log.warning(\"[{}] {}:\\n{}\".format(idx, station, e))\n",
      "        \n",
      "obs_data = DataFrame.from_dict(data)\n",
      "\n",
      "# Split good and bad stations.\n",
      "pattern = '|'.join(bad_station)\n",
      "if pattern:\n",
      "    all_obs['bad_station'] = all_obs.station.str.contains(pattern)\n",
      "else:\n",
      "    all_obs['bad_station'] = ~all_obs.station.str.contains(pattern)\n",
      "\n",
      "# Save updated `all_obs.csv`.\n",
      "fname = '{}-all_obs.csv'.format(run_name)\n",
      "fname = os.path.join(run_name, fname)\n",
      "all_obs.to_csv(fname)\n",
      "\n",
      "comment = \"Several stations from http://opendap.co-ops.nos.noaa.gov\"\n",
      "kw = dict(longitude=all_obs.lon,\n",
      "          latitude=all_obs.lat,\n",
      "          station_attr=dict(cf_role=\"timeseries_id\"),\n",
      "          cube_attr=dict(featureType='timeSeries',\n",
      "                         Conventions='CF-1.6',\n",
      "                         standard_name_vocabulary='CF-1.6',\n",
      "                         cdm_data_type=\"Station\",\n",
      "                         comment=comment,\n",
      "                         url=url))\n",
      "\n",
      "save_timeseries(obs_data, outfile=outfile,\n",
      "                standard_name=sos_name, **kw)\n",
      "\n",
      "to_html(obs_data.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loop discovered models and save the nearest time-series"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import warnings\n",
      "from iris.exceptions import (CoordinateNotFoundError, ConstraintMismatchError,\n",
      "                             MergeError)\n",
      "from utilities import (TimeoutException, quick_load_cubes, proc_cube,\n",
      "                       time_limit, is_model, get_model_name, get_surface)\n",
      "\n",
      "log.info(fmt(' Models '))\n",
      "cubes = dict()\n",
      "with warnings.catch_warnings():\n",
      "    warnings.simplefilter(\"ignore\")  # Suppress iris warnings.\n",
      "    for k, url in enumerate(dap_urls):\n",
      "        log.info('\\n[Reading url {}/{}]: {}'.format(k+1, len(dap_urls), url))\n",
      "        try:\n",
      "            with time_limit(60*5):\n",
      "                cube = quick_load_cubes(url, name_list,\n",
      "                                        callback=None, strict=True)\n",
      "                if is_model(cube):\n",
      "                    cube = proc_cube(cube, bbox=bbox,\n",
      "                                     time=(start, stop), units=units)\n",
      "                else:\n",
      "                    log.warning(\"[Not model data]: {}\".format(url))\n",
      "                    continue\n",
      "                cube = get_surface(cube)\n",
      "                mod_name, model_full_name = get_model_name(cube, url)\n",
      "                cubes.update({mod_name: cube})\n",
      "        except (TimeoutException, RuntimeError, ValueError,\n",
      "                ConstraintMismatchError, CoordinateNotFoundError,\n",
      "                IndexError) as e:\n",
      "            log.warning('Cannot get cube for: {}\\n{}'.format(url, e))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from iris.pandas import as_series\n",
      "from utilities import (make_tree, get_nearest_water,\n",
      "                       add_station, ensure_timeseries, remove_ssh)\n",
      "\n",
      "for mod_name, cube in cubes.items():\n",
      "    fname = '{:%Y-%m-%d}-{}.nc'.format(stop, mod_name)\n",
      "    fname = os.path.join(run_name, fname)\n",
      "    log.info(fmt(' Downloading to file {} '.format(fname)))\n",
      "    try:\n",
      "        tree, lon, lat = make_tree(cube)\n",
      "    except CoordinateNotFoundError as e:\n",
      "        log.warning('Cannot make KDTree for: {}'.format(mod_name))\n",
      "        continue\n",
      "    # Get model series at observed locations.\n",
      "    raw_series = dict()\n",
      "    for station, obs in all_obs.iterrows():\n",
      "        try:\n",
      "            kw = dict(k=10, max_dist=0.04, min_var=0.01)\n",
      "            args = cube, tree, obs.lon, obs.lat\n",
      "            series, dist, idx = get_nearest_water(*args, **kw)\n",
      "        except ValueError as e:\n",
      "            status = \"No Data\"\n",
      "            log.info('[{}] {}'.format(status, obs.name))\n",
      "            continue\n",
      "        if not series:\n",
      "            status = \"Land   \"\n",
      "        else:\n",
      "            raw_series.update({obs['station']: series})\n",
      "            series = as_series(series)\n",
      "            status = \"Water  \"\n",
      "        log.info('[{}] {}'.format(status, obs.name))\n",
      "    if raw_series:  # Save cube.\n",
      "        for station, cube in raw_series.items():\n",
      "            cube = add_station(cube, station)\n",
      "            cube = remove_ssh(cube)\n",
      "        try:\n",
      "            cube = iris.cube.CubeList(raw_series.values()).merge_cube()\n",
      "        except MergeError as e:\n",
      "            log.warning(e)\n",
      "        ensure_timeseries(cube)\n",
      "        iris.save(cube, fname)\n",
      "        del cube\n",
      "    log.info('Finished processing [{}]: {}\\n'.format(mod_name, url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for mod_name, cube in cubes.items()[3:4]:\n",
      "    fname = '{:%Y-%m-%d}-{}.nc'.format(stop, mod_name)\n",
      "    fname = os.path.join(run_name, fname)\n",
      "    log.info(fmt(' Downloading to file {} '.format(fname)))\n",
      "    log.info(fmt('{}'.format(url)))\n",
      "    try:\n",
      "        tree, lon, lat = make_tree(cube)\n",
      "    except CoordinateNotFoundError as e:\n",
      "        log.warning('Cannot make KDTree for: {}'.format(mod_name))\n",
      "        continue\n",
      "    # Get model series at observed locations.\n",
      "    raw_series = dict()\n",
      "    for station, obs in all_obs.iterrows():\n",
      "        try:\n",
      "            kw = dict(k=10, max_dist=0.04, min_var=0.01)\n",
      "            args = cube, tree, obs.lon, obs.lat\n",
      "            series, dist, idx = get_nearest_water(*args, **kw)\n",
      "        except ValueError as e:\n",
      "            status = \"No Data\"\n",
      "            log.info('[{}] {}'.format(status, obs.name))\n",
      "            continue\n",
      "        if not series:\n",
      "            status = \"Land   \"\n",
      "        else:\n",
      "            raw_series.update({obs['station']: series})\n",
      "            series = as_series(series)\n",
      "            status = \"Water  \"\n",
      "        log.info('[{}] {}'.format(status, obs.name))\n",
      "    if raw_series:  # Save cube.\n",
      "        for station, cube in raw_series.items():\n",
      "            cube = add_station(cube, station)\n",
      "            cube = remove_ssh(cube)\n",
      "        try:\n",
      "            cube = iris.cube.CubeList(raw_series.values()).merge_cube()\n",
      "        except MergeError as e:\n",
      "            log.warning(e)\n",
      "        ensure_timeseries(cube)\n",
      "        iris.save(cube, fname)\n",
      "        del cube\n",
      "    log.info('Finished processing [{}]'.format(url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "elapsed = time.time() - start_time\n",
      "log.info('{:.2f} minutes'.format(elapsed/60.))\n",
      "log.info('EOF')\n",
      "\n",
      "with open('{}/log.txt'.format(run_name)) as f:\n",
      "    print(f.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}